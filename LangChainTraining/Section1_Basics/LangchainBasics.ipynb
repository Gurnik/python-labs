{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9bbe13",
   "metadata": {},
   "source": [
    "## Installation required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da83ba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Using cached langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.4.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.42-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached orjson-3.11.1-cp313-cp313-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp313-cp313-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.2.3-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.72-py3-none-any.whl (442 kB)\n",
      "Using cached langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Using cached langsmith-0.4.10-py3-none-any.whl (372 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Using cached sqlalchemy-2.0.42-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached greenlet-3.2.3-cp313-cp313-win_amd64.whl (297 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.11.1-cp313-cp313-win_amd64.whl (131 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached zstandard-0.23.0-cp313-cp313-win_amd64.whl (495 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, sniffio, PyYAML, pydantic-core, orjson, jsonpointer, h11, greenlet, annotated-types, SQLAlchemy, requests-toolbelt, pydantic, jsonpatch, httpcore, anyio, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.42 annotated-types-0.7.0 anyio-4.10.0 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.72 langchain-text-splitters-0.3.9 langsmith-0.4.10 orjson-3.11.1 pydantic-2.11.7 pydantic-core-2.33.2 requests-toolbelt-1.0.0 sniffio-1.3.1 typing-inspection-0.4.1 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Using cached langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting ollama<1.0.0,>=0.5.1 (from langchain-ollama)\n",
      "  Using cached ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-ollama) (0.3.72)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.4.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.11.7)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from ollama<1.0.0,>=0.5.1->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kandd\\onedrive\\documents\\fullstack\\udemy\\karthik kk\\langchaintraining\\myenv313\\lib\\site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.3.1)\n",
      "Using cached langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
      "Using cached ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-0.3.6 ollama-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-ollama\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479b0ed",
   "metadata": {},
   "source": [
    "### Intracting with LLM\n",
    "\n",
    "<img src=\"./Images/LocalLLM.png\" width=\"800\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755067cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\nOkay, the user greeted me with a simple \"Hello\" and asked how I\\'m doing. Hmm, this is a pretty casual opening—it\\'s not complex or technical, so they\\'re probably just starting a conversation to test the waters or maybe they\\'re in a lighthearted mood.\\n\\nFirst, let\\'s break down what might be behind this message. The user could be:\\n- Just curious about how an AI assistant handles social interactions\\n- Wanting small talk before diving into their actual request\\n- Possibly lonely and seeking some engagement (though I shouldn\\'t assume that)\\n- Or maybe they\\'re new to chatting with AIs and want to understand the dynamic\\n\\nThe tone is friendly, so my response should match. \"Hello! 😊\" feels right—it\\'s warm but professional. The second part about not having feelings keeps it honest while acknowledging their question. \\n\\nI wonder what they\\'ll ask next though... Since this seems like a general greeting, I shouldn\\'t overthink it too much, just keep it simple and inviting. Maybe they\\'re the type who prefers straightforward answers first. \\n\\nThe smiley emoji adds approachability without being overly familiar. It strikes that balance between human-like interaction and maintaining professional boundaries.\\n</think>\\nHello! 😊  \\nI\\'m an AI assistant here to help you with any questions or tasks, so I don\\'t have feelings like humans do—but I’m fully operational and ready to assist you however I can.  \\n\\nHow are *you* doing today?' additional_kwargs={} response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-08-03T07:32:49.4920672Z', 'done': True, 'done_reason': 'stop', 'total_duration': 31962854100, 'load_duration': 5711835000, 'prompt_eval_count': 10, 'prompt_eval_duration': 554930100, 'eval_count': 302, 'eval_duration': 25688596300, 'model_name': 'deepseek-r1:8b'} id='run--b8699b88-47f5-4863-b139-8c091c687487-0' usage_metadata={'input_tokens': 10, 'output_tokens': 302, 'total_tokens': 312}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "  base_url=\"http://localhost:11434\",\n",
    "  model = \"deepseek-r1:8b\",\n",
    "  temperature=0.5,\n",
    "  max_tokens = 250,\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Hello, how are you doing today?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ffa6e",
   "metadata": {},
   "source": [
    "### Load ENV file will connect LangSmith\n",
    "\n",
    "<img src=\"./Images/LocalLLMWithLangSmith.png\" width=\"800\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb772c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "ExecuteAutomationLangchainTraining\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load = load_dotenv('./../.env')\n",
    "\n",
    "# print(os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "# print(os.getenv(\"LANGSMITH_PROJECT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc95927",
   "metadata": {},
   "source": [
    "## Prompt & Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae188ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the advantage of running the LLM in local machine'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"What is the advantage of running the LLM in {env}\");\n",
    "\n",
    "prompt = prompt_template.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "content = llm.invoke(prompt).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d71ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the advantages of running an LLM locally. Let me start by understanding their query. They might be considering whether to use a local setup or cloud-based services for AI tasks.\n",
      "\n",
      "First, I need to think about why someone would run an LLM locally. The main points are privacy, control, and offline capabilities. But maybe the user is concerned about data security because they handle sensitive information. Or perhaps they're in an environment with poor internet connectivity. \n",
      "\n",
      "They might be a developer or researcher who needs customization. Maybe their existing cloud services don't offer enough flexibility for integrating specific models or tweaking parameters. They could also be someone interested in experimenting with LLMs without relying on third-party APIs, which are often expensive.\n",
      "\n",
      "Another angle is cost-effectiveness. If they're using cloud services now and facing high bills due to large API calls, local might save money long-term. But the initial hardware setup costs can be steep—like GPUs and RAM. The user might not realize that unless they have existing infrastructure or plan to invest, it could still be costly upfront.\n",
      "\n",
      "Performance is another factor. Latency issues with cloud services are a pain for real-time apps. If their use case requires instant responses, local processing would help. But if the model isn't optimized for their hardware, performance might not meet expectations. They should ensure compatibility though.\n",
      "\n",
      "Control over data and models gives them full ownership. Maybe they're in compliance-sensitive industries like healthcare or finance where data sovereignty is crucial. Or perhaps they want to train on proprietary datasets without exposing them to external servers.\n",
      "\n",
      "Cost savings through avoiding API fees are significant for frequent users. But if their usage is sporadic, maybe the local setup isn't worth it yet. They need to weigh how often they'll use the model versus the hardware investment.\n",
      "\n",
      "Customizability allows modifications and integration into existing systems. The user might have specific needs not covered by standard cloud services—domain-specific adjustments or unique workflows. This could be key for their project's uniqueness.\n",
      "\n",
      "Offline capabilities are a big plus if internet is unreliable or restricted. Maybe they're in remote areas, working on embedded devices without connectivity, or need to function in air-gap environments like military settings.\n",
      "\n",
      "The user might not have considered the technical requirements—like needing powerful hardware and specialized software skills. They could be underestimating the setup complexity. Also, model size matters; smaller models are feasible locally, but larger ones require significant resources.\n",
      "\n",
      "Are they looking for a balance between local and cloud? Maybe hybrid approaches exist, but their question is focused on local benefits. Their deeper need might be about security, cost efficiency, or needing full control without external dependencies.\n",
      "\n",
      "They probably want to know if the effort of setting up a local environment is justified by these advantages. So I should highlight the key benefits while addressing potential hidden costs like hardware investment and maintenance overhead.\n",
      "</think>\n",
      "Running an LLM (Large Language Model) locally on your own machine offers several distinct advantages compared to relying solely on cloud-based APIs or services:\n",
      "\n",
      "1.  **Enhanced Privacy and Data Security:**\n",
      "    *   **Data Confidentiality:** Your input data never leaves your device, reducing the risk of sensitive information being exposed over the network or stored externally by third-party providers.\n",
      "    *   **Reduced Attack Surface:** You control the local environment where the model processes data. While still vulnerable to local threats (malware), it avoids the risks associated with transmitting data across networks and trusting external servers.\n",
      "\n",
      "2.  **Full Control and Customization:**\n",
      "    *   **Model Selection & Tuning:** You can choose which specific LLM architecture or version you want, potentially even fine-tuning it on your own hardware for domain-specific needs (e.g., legal, medical language).\n",
      "    *   **Configuration Flexibility:** You have complete control over parameters like temperature, top-p, max tokens, etc. This is crucial if the cloud provider's API doesn't offer sufficient granularity or specific options you require.\n",
      "    *   **Integration Capabilities:** Easier to integrate with other local software applications, databases, APIs, or custom hardware without network intermediaries.\n",
      "\n",
      "3.  **Cost-Effectiveness (for frequent users):**\n",
      "    *   **No Pay-per-use/API Fees:** While the initial hardware cost is high, if you use the model frequently and heavily, running it locally can be significantly cheaper than paying for repeated API calls to cloud providers.\n",
      "    *   **Reduced Cloud Costs:** Avoids ongoing subscription fees or hourly charges associated with cloud instances hosting LLM inference.\n",
      "\n",
      "4.  **Lower Latency:**\n",
      "    *   **Faster Response Times:** Processing requests entirely on the local machine eliminates network delays (latency) inherent in cloud-based solutions, which can be critical for real-time interactions or applications requiring immediate feedback.\n",
      "\n",
      "5.  **Ownership and Transparency:**\n",
      "    *   **Full Model Ownership:** You run your own copy of the model. This means you know exactly what software version is being used.\n",
      "    *   (Potentially) More Transparent Operation:** While not always easy, running locally gives more insight into how the specific instance is configured and operating compared to a black-box cloud API.\n",
      "\n",
      "6.  **Accessibility and Availability:**\n",
      "    *   **Offline Capability:** You can use the LLM even when you don't have internet access or in environments with restricted network connectivity.\n",
      "    *   **Predictable Performance:** Usage isn't affected by external factors like third-party service outages, rate limiting, or fluctuating performance levels.\n",
      "\n",
      "7.  **Customizability for Specific Hardware:**\n",
      "    *   You can potentially optimize the model's inference code and settings specifically for your hardware (CPU, GPU make/model) to squeeze out maximum performance.\n",
      "\n",
      "**Important Considerations & Trade-offs:**\n",
      "\n",
      "*   **Hardware Requirements:** Running LLMs locally requires significant computational power, typically a powerful GPU with sufficient VRAM. Large models may also require substantial RAM.\n",
      "*   **Software Complexity:** It involves installing and managing complex software stacks (operating system compatibility layers like WSL/WGPU on Windows, Python environments, model repositories).\n",
      "*   **Model Size & Resource Usage:** Full-sized LLMs are very large files (often tens or hundreds of GB). Running them requires not only the hardware but also storage space. Smaller models can be run locally more easily.\n",
      "*   **Updates & Maintenance:** You need to manage updates for both your hardware drivers and the software framework hosting the model, unlike relying on a vendor-provided API which handles these automatically.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The primary advantage is control – over data privacy, usage costs (long-term), customization, integration, and offline availability. Latency reduction is also significant. However, this comes at the cost of substantial initial hardware investment and technical expertise to set up and maintain the local environment. For users prioritizing complete self-sufficiency, handling sensitive data internally, needing specific customizations, or using the model frequently in an offline context, a local setup can be highly beneficial.\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10be5e",
   "metadata": {},
   "source": [
    "#### Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1499a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, user asked about advantages of running AI models locally. Let me break this down carefully.\n",
      "\n",
      "First, I need to understand what they really mean by \"local\" - probably their own computer or a small on-premises setup rather than cloud services. The question seems practical, maybe from someone evaluating deployment options for work or personal projects.\n",
      "\n",
      "Hmm, the user might be comparing local vs cloud AI model execution and weighing pros and cons. They didn't specify if they're an individual developer or part of an organization, but I should cover both angles since it's relevant to budget constraints too.\n",
      "\n",
      "Let me structure this systematically. The main advantage is control - that's probably their biggest concern. Then data privacy matters for sensitive applications like healthcare or finance. Customization flexibility could be important if they want specialized hardware integration.\n",
      "\n",
      "I wonder if they're asking because they're frustrated with cloud costs? Many people hit usage limits when experimenting locally first. Or maybe latency issues in real-time applications? That's worth mentioning too.\n",
      "\n",
      "The response should balance technical benefits (like GPU control) and practical considerations (cost of high-end hardware). Should include examples to make it concrete - like how a video processing app needs different resources than text generation.\n",
      "\n",
      "Need to be careful not to oversell local execution though. It has trade-offs, especially for large models. Maybe they're underestimating the hardware requirements? The table format would help visualize this clearly.\n",
      "\n",
      "Oh! I should emphasize that \"local\" doesn't mean just any computer - it implies having proper infrastructure like decent GPUs and enough RAM. That's a common misconception.\n",
      "</think>\n",
      "Running AI models locally (on your own machine) offers several distinct advantages, primarily centered around control, privacy, accessibility, and performance for certain use cases:\n",
      "\n",
      "| **Advantage** | **Key Benefit** | **Example Use Case** |\n",
      "|---------------|-----------------|---------------------|\n",
      "| **Complete Control & Flexibility** | Direct access to model execution without third-party restrictions | Modifying open-source models or implementing custom algorithms not available through cloud services |\n",
      "| **Data Privacy and Confidentiality** | Ensures sensitive data stays on local hardware, away from network transmission risks | Healthcare applications that process patient records or financial systems handling proprietary data |\n",
      "| **Accessibility & No Internet Dependency** | Works even without internet connectivity; eliminates need for constant API calls to cloud services | Running AI models in remote locations with limited or unreliable internet access (e.g., field operations) |\n",
      "| **Potentially Lower Costs (for some)** | Avoids ongoing subscription fees from cloud providers, though requires significant upfront hardware investment | Small businesses or startups operating on tight budgets that can't afford premium cloud tiers |\n",
      "\n",
      "Here's a deeper look at each of these advantages:\n",
      "\n",
      "1.  **Complete Control & Flexibility:**\n",
      "    *   **Direct Execution:** You run the model yourself, giving you full control over its execution environment (software versions, libraries, dependencies). This avoids potential issues with third-party service interruptions or API changes.\n",
      "    *   **Modification and Experimentation:** Easier to modify open-source models locally for specific tasks. You can fine-tune them on custom datasets, experiment with different architectures, or implement unique algorithms without relying on cloud provider tools.\n",
      "    *   **Bypass Cloud Quotas/Limits:** Local execution avoids the constraints and potential throttling associated with free tiers or paid plans from cloud providers.\n",
      "\n",
      "2.  **Data Privacy and Confidentiality:**\n",
      "    *   **On-Premise Data Handling:** Sensitive data (customer information, medical records, financial data, intellectual property) can be kept entirely on your local machine or internal network.\n",
      "    *   **Reduced Network Risks:** Eliminates the risk of transmitting raw data to a third-party cloud server. This is crucial for compliance with regulations like GDPR, HIPAA, etc., and protects trade secrets.\n",
      "\n",
      "3.  **Accessibility & No Internet Dependency:**\n",
      "    *   **Offline Capability:** Your application can run AI features even without an internet connection or in low-connectivity environments.\n",
      "    *   **Reduced Latency (Potentially):** For some tasks, processing data locally means less delay compared to sending it online and receiving the result. This is particularly relevant for real-time applications like voice assistants on a local device.\n",
      "\n",
      "4.  **Performance Optimization:**\n",
      "    *   **Tailored Hardware:** You can optimize the model specifically for your hardware (e.g., using specific GPU libraries or quantization techniques). While cloud providers offer good performance, sometimes tailoring it to *your exact* CPU/GPU combination yields better results locally.\n",
      "    *   **Avoiding Network Bandwidth/Overhead:** For large models and heavy computation, transferring data back and forth from the cloud adds network latency and consumes significant bandwidth. Running entirely on local hardware avoids this overhead.\n",
      "\n",
      "5.  **Potentially Lower Costs (for some scenarios):**\n",
      "    *   Avoids ongoing subscription fees if running a model frequently or using high-end services.\n",
      "    *   While requiring significant upfront investment in powerful hardware (especially GPUs), the total cost of ownership might be lower for frequent, dedicated use compared to paying cloud provider tiers.\n",
      "\n",
      "6.  **Reproducibility:**\n",
      "    *   Having everything on your local machine makes it easier to reproduce results and ensure consistency across different runs or deployment environments without relying on external systems' availability or configuration drift.\n",
      "\n",
      "**Important Caveat:** \"Local\" doesn't necessarily mean \"cheap.\" Running complex AI models, especially large language models (LLMs) or real-time vision models, typically requires powerful hardware (good CPU, multiple high-memory GPUs). The cost of acquiring and maintaining such hardware can be substantial. Therefore, the advantage lies more in control, privacy, accessibility *when* you have that local infrastructure.\n",
      "\n",
      "In summary, running AI models locally is ideal when you need maximum control over your data and environment, absolute offline capability is required, or you want to run computationally intensive tasks with minimal network overhead.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "\n",
    "systemMessage = SystemMessagePromptTemplate.from_template(\"You are an LLM expert\")\n",
    "humanMessage = HumanMessagePromptTemplate.from_template(\"What is the advantage of running AI Models in {env}\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    systemMessage, \n",
    "    humanMessage\n",
    "])\n",
    "\n",
    "# 1 way of doing it\n",
    "# prompt_template = ChatPromptTemplate([\n",
    "#     (\"system\", \"You are an LLM expert\"),\n",
    "#     (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "# ])\n",
    "\n",
    "prompt_template\n",
    "\n",
    "prompt = prompt_template.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "content = llm.invoke(prompt).content\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813ec70",
   "metadata": {},
   "source": [
    "#### MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22dc7abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "\n",
      "Okay\n",
      ",\n",
      " user\n",
      " is\n",
      " asking\n",
      " about\n",
      " advantages\n",
      " of\n",
      " running\n",
      " L\n",
      "LM\n",
      "s\n",
      " locally\n",
      ".\n",
      " This\n",
      " seems\n",
      " like\n",
      " a\n",
      " practical\n",
      " question\n",
      " from\n",
      " someone\n",
      " evaluating\n",
      " whether\n",
      " to\n",
      " deploy\n",
      " models\n",
      " on\n",
      " their\n",
      " own\n",
      " hardware\n",
      " or\n",
      " rely\n",
      " on\n",
      " cloud\n",
      " services\n",
      ".\n",
      "\n",
      "\n",
      "Hmm\n",
      ",\n",
      " let\n",
      "'s\n",
      " break\n",
      " this\n",
      " down\n",
      " carefully\n",
      ".\n",
      " The\n",
      " user\n",
      " might\n",
      " be\n",
      " considering\n",
      " self\n",
      "-host\n",
      "ing\n",
      " an\n",
      " AI\n",
      " assistant\n",
      ",\n",
      " doing\n",
      " research\n",
      " offline\n",
      ",\n",
      " or\n",
      " working\n",
      " with\n",
      " sensitive\n",
      " data\n",
      ".\n",
      " They\n",
      " didn\n",
      "'t\n",
      " specify\n",
      " use\n",
      " case\n",
      ",\n",
      " but\n",
      " the\n",
      " general\n",
      " question\n",
      " suggests\n",
      " they\n",
      "'re\n",
      " weighing\n",
      " privacy\n",
      " and\n",
      " control\n",
      " factors\n",
      " against\n",
      " convenience\n",
      ".\n",
      "\n",
      "\n",
      "First\n",
      " thought\n",
      ":\n",
      " local\n",
      " execution\n",
      " fundamentally\n",
      " changes\n",
      " the\n",
      " risk\n",
      " profile\n",
      " compared\n",
      " to\n",
      " cloud\n",
      " services\n",
      ".\n",
      " For\n",
      " someone\n",
      " handling\n",
      " personal\n",
      " data\n",
      " or\n",
      " proprietary\n",
      " work\n",
      ",\n",
      " this\n",
      " could\n",
      " be\n",
      " crucial\n",
      ".\n",
      " But\n",
      " for\n",
      " casual\n",
      " users\n",
      ",\n",
      " it\n",
      " might\n",
      " not\n",
      " matter\n",
      " as\n",
      " much\n",
      ".\n",
      "\n",
      "\n",
      "I\n",
      " should\n",
      " structure\n",
      " this\n",
      " by\n",
      " starting\n",
      " with\n",
      " security\n",
      "/\n",
      "privacy\n",
      " -\n",
      " that\n",
      "'s\n",
      " probably\n",
      " their\n",
      " biggest\n",
      " concern\n",
      " if\n",
      " they\n",
      "'re\n",
      " asking\n",
      " about\n",
      " local\n",
      " vs\n",
      " remote\n",
      " options\n",
      ".\n",
      " Then\n",
      " move\n",
      " to\n",
      " customization\n",
      " and\n",
      " control\n",
      " aspects\n",
      ",\n",
      " which\n",
      " are\n",
      " more\n",
      " technical\n",
      " but\n",
      " still\n",
      " important\n",
      ".\n",
      " Finally\n",
      " cover\n",
      " offline\n",
      " capabilities\n",
      " since\n",
      " that\n",
      "'s\n",
      " a\n",
      " major\n",
      " pain\n",
      " point\n",
      " for\n",
      " mobile\n",
      " users\n",
      " or\n",
      " those\n",
      " in\n",
      " unstable\n",
      " networks\n",
      ".\n",
      "\n",
      "\n",
      "Wait\n",
      ",\n",
      " the\n",
      " user\n",
      " might\n",
      " be\n",
      " comparing\n",
      " different\n",
      " L\n",
      "LM\n",
      " deployment\n",
      " methods\n",
      ".\n",
      " Maybe\n",
      " I\n",
      " should\n",
      " subtly\n",
      " include\n",
      " how\n",
      " this\n",
      " compares\n",
      " to\n",
      " cloud\n",
      " APIs\n",
      " while\n",
      " answering\n",
      " about\n",
      " local\n",
      " execution\n",
      " -\n",
      " that\n",
      " way\n",
      " they\n",
      " get\n",
      " context\n",
      " without\n",
      " explicitly\n",
      " asking\n",
      " for\n",
      " comparisons\n",
      ".\n",
      "\n",
      "\n",
      "Also\n",
      " need\n",
      " to\n",
      " consider\n",
      " their\n",
      " technical\n",
      " level\n",
      ".\n",
      " They\n",
      " didn\n",
      "'t\n",
      " ask\n",
      " about\n",
      " implementation\n",
      " details\n",
      ",\n",
      " so\n",
      " I\n",
      "'ll\n",
      " keep\n",
      " explanations\n",
      " accessible\n",
      " but\n",
      " not\n",
      " overs\n",
      "implify\n",
      " the\n",
      " underlying\n",
      " concepts\n",
      " like\n",
      " parameter\n",
      " tuning\n",
      " or\n",
      " quant\n",
      "ization\n",
      ".\n",
      "\n",
      "\n",
      "The\n",
      " answer\n",
      " should\n",
      " balance\n",
      " being\n",
      " comprehensive\n",
      " with\n",
      " concise\n",
      " -\n",
      " list\n",
      " key\n",
      " benefits\n",
      " but\n",
      " explain\n",
      " why\n",
      " each\n",
      " matters\n",
      ".\n",
      " User\n",
      " probably\n",
      " wants\n",
      " concrete\n",
      " reasons\n",
      ",\n",
      " not\n",
      " just\n",
      " \"\n",
      "you\n",
      " can\n",
      " control\n",
      " your\n",
      " data\n",
      "\"\n",
      " without\n",
      " nu\n",
      "ance\n",
      ".\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "Running\n",
      " a\n",
      " Large\n",
      " Language\n",
      " Model\n",
      " (\n",
      "LL\n",
      "M\n",
      ")\n",
      " locally\n",
      " on\n",
      " your\n",
      " machine\n",
      " offers\n",
      " several\n",
      " distinct\n",
      " advantages\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " \n",
      " **\n",
      "Privacy\n",
      " and\n",
      " Data\n",
      " Security\n",
      ":\n",
      "**\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "Data\n",
      " Confidential\n",
      "ity\n",
      ":**\n",
      " Your\n",
      " input\n",
      " prompts\n",
      " and\n",
      " the\n",
      " model\n",
      "'s\n",
      " processing\n",
      " of\n",
      " them\n",
      " stay\n",
      " entirely\n",
      " on\n",
      " your\n",
      " local\n",
      " hardware\n",
      ",\n",
      " never\n",
      " leaving\n",
      " your\n",
      " device\n",
      " or\n",
      " network\n",
      ".\n",
      " This\n",
      " is\n",
      " crucial\n",
      " for\n",
      " sensitive\n",
      " information\n",
      " like\n",
      " personal\n",
      " data\n",
      ",\n",
      " code\n",
      ",\n",
      " financial\n",
      " details\n",
      ",\n",
      " medical\n",
      " records\n",
      ",\n",
      " or\n",
      " confidential\n",
      " business\n",
      " documents\n",
      ".\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "Reduc\n",
      "ed\n",
      " Exposure\n",
      " to\n",
      " External\n",
      " Servers\n",
      ":**\n",
      " You\n",
      " avoid\n",
      " sending\n",
      " potentially\n",
      " large\n",
      " amounts\n",
      " of\n",
      " data\n",
      " over\n",
      " the\n",
      " internet\n",
      " to\n",
      " third\n",
      "-party\n",
      " cloud\n",
      " providers\n",
      ".\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " \n",
      " **\n",
      "Control\n",
      " and\n",
      " Custom\n",
      "ization\n",
      ":\n",
      "**\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "Parameter\n",
      " Tun\n",
      "ing\n",
      ":**\n",
      " You\n",
      " have\n",
      " full\n",
      " control\n",
      " over\n",
      " the\n",
      " model\n",
      "'s\n",
      " parameters\n",
      " (\n",
      "like\n",
      " temperature\n",
      " for\n",
      " creativity\n",
      ",\n",
      " top\n",
      "-p\n",
      " for\n",
      " diversity\n",
      ").\n",
      " This\n",
      " allows\n",
      " you\n",
      " to\n",
      " fine\n",
      "-t\n",
      "une\n",
      " it\n",
      " precisely\n",
      " for\n",
      " your\n",
      " specific\n",
      " application\n",
      " or\n",
      " desired\n",
      " output\n",
      " style\n",
      ".\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "Prompt\n",
      " Engineering\n",
      " Flex\n",
      "ibility\n",
      ":**\n",
      " You\n",
      " can\n",
      " experiment\n",
      " extensively\n",
      " with\n",
      " prompts\n",
      " without\n",
      " relying\n",
      " on\n",
      " external\n",
      " APIs\n",
      " having\n",
      " rate\n",
      " limits\n",
      " or\n",
      " specific\n",
      " formats\n",
      " they\n",
      " expect\n",
      ".\n",
      " Iter\n",
      "ation\n",
      " is\n",
      " faster\n",
      " and\n",
      " more\n",
      " direct\n",
      ".\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "Accessibility\n",
      ":**\n",
      " The\n",
      " model\n",
      " runs\n",
      " entirely\n",
      " offline\n",
      ",\n",
      " making\n",
      " its\n",
      " capabilities\n",
      " available\n",
      " whenever\n",
      " you\n",
      " need\n",
      " them\n",
      ",\n",
      " even\n",
      " if\n",
      " you\n",
      " have\n",
      " no\n",
      " internet\n",
      " connection\n",
      ".\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " \n",
      " **\n",
      "Ownership\n",
      ":\n",
      "**\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " You\n",
      " own\n",
      " the\n",
      " instance\n",
      " of\n",
      " the\n",
      " model\n",
      " running\n",
      " locally\n",
      " (\n",
      "or\n",
      " at\n",
      " least\n",
      " control\n",
      " your\n",
      " specific\n",
      " setup\n",
      ").\n",
      " There\n",
      " are\n",
      " no\n",
      " subscription\n",
      " fees\n",
      " or\n",
      " recurring\n",
      " costs\n",
      " associated\n",
      " with\n",
      " using\n",
      " a\n",
      " cloud\n",
      " API\n",
      " for\n",
      " each\n",
      " interaction\n",
      ".\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " Potential\n",
      " for\n",
      " offline\n",
      " use\n",
      " without\n",
      " needing\n",
      " an\n",
      " active\n",
      " service\n",
      " provider\n",
      ",\n",
      " although\n",
      " you\n",
      " still\n",
      " need\n",
      " to\n",
      " pay\n",
      " for\n",
      " the\n",
      " hardware\n",
      ".\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " \n",
      " **\n",
      "Performance\n",
      " Optimization\n",
      ":\n",
      "**\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "Reduc\n",
      "ed\n",
      " Lat\n",
      "ency\n",
      ":**\n",
      " Inter\n",
      "actions\n",
      " can\n",
      " be\n",
      " significantly\n",
      " faster\n",
      " since\n",
      " there\n",
      "'s\n",
      " no\n",
      " network\n",
      " delay\n",
      " between\n",
      " your\n",
      " machine\n",
      " and\n",
      " external\n",
      " servers\n",
      " (\n",
      "though\n",
      " depends\n",
      " on\n",
      " local\n",
      " hardware\n",
      " speed\n",
      ").\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "Band\n",
      "width\n",
      " Efficiency\n",
      ":**\n",
      " While\n",
      " large\n",
      " context\n",
      " windows\n",
      " require\n",
      " significant\n",
      " RAM\n",
      ",\n",
      " they\n",
      " consume\n",
      " much\n",
      " less\n",
      " network\n",
      " bandwidth\n",
      " than\n",
      " cloud\n",
      " API\n",
      " calls\n",
      ".\n",
      "\n",
      "\n",
      "5\n",
      ".\n",
      " \n",
      " **\n",
      "Offline\n",
      " Use\n",
      ":\n",
      "**\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " As\n",
      " mentioned\n",
      " above\n",
      ",\n",
      " this\n",
      " is\n",
      " a\n",
      " major\n",
      " benefit\n",
      " for\n",
      " environments\n",
      " without\n",
      " reliable\n",
      " internet\n",
      " access\n",
      " or\n",
      " where\n",
      " online\n",
      " interaction\n",
      " isn\n",
      "'t\n",
      " desired\n",
      " (\n",
      "e\n",
      ".g\n",
      ".,\n",
      " research\n",
      " in\n",
      " remote\n",
      " locations\n",
      ").\n",
      "\n",
      "\n",
      "**\n",
      "However\n",
      ",\n",
      " it\n",
      "'s\n",
      " important\n",
      " to\n",
      " note\n",
      " the\n",
      " trade\n",
      "-offs\n",
      " involved\n",
      " with\n",
      " local\n",
      " execution\n",
      " compared\n",
      " to\n",
      " using\n",
      " cloud\n",
      " APIs\n",
      ":\n",
      "**\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " \n",
      " **\n",
      "Hardware\n",
      " Requirements\n",
      ":**\n",
      " Running\n",
      " large\n",
      " L\n",
      "LM\n",
      "s\n",
      " locally\n",
      " requires\n",
      " significant\n",
      " computational\n",
      " resources\n",
      ":\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "RAM\n",
      ":**\n",
      " Needed\n",
      " to\n",
      " store\n",
      " the\n",
      " model\n",
      " and\n",
      " its\n",
      " context\n",
      " window\n",
      ".\n",
      " Larger\n",
      " models\n",
      " need\n",
      " more\n",
      " RAM\n",
      ".\n",
      "\n",
      "   \n",
      " *\n",
      "  \n",
      " **\n",
      "CPU\n",
      "/G\n",
      "PU\n",
      " Power\n",
      ":**\n",
      " Essential\n",
      " for\n",
      " processing\n",
      " speed\n",
      ",\n",
      " especially\n",
      " with\n",
      " complex\n",
      " operations\n",
      " like\n",
      " attention\n",
      " mechanisms\n",
      ".\n",
      " GPUs\n",
      " are\n",
      " highly\n",
      " recommended\n",
      " (\n",
      "and\n",
      " often\n",
      " required\n",
      ")\n",
      " for\n",
      " decent\n",
      " performance\n",
      ".\n",
      "\n",
      "2\n",
      ".\n",
      " \n",
      " **\n",
      "Resource\n",
      " Consumption\n",
      ":**\n",
      " A\n",
      " local\n",
      " L\n",
      "LM\n",
      " instance\n",
      " consumes\n",
      " continuous\n",
      " resources\n",
      " (\n",
      "RAM\n",
      ",\n",
      " GPU\n",
      " memory\n",
      ",\n",
      " CPU\n",
      " power\n",
      "),\n",
      " even\n",
      " when\n",
      " idle\n",
      " or\n",
      " not\n",
      " actively\n",
      " generating\n",
      " text\n",
      ",\n",
      " which\n",
      " can\n",
      " impact\n",
      " other\n",
      " tasks\n",
      " on\n",
      " the\n",
      " machine\n",
      " unless\n",
      " properly\n",
      " managed\n",
      ".\n",
      "\n",
      "3\n",
      ".\n",
      " \n",
      " **\n",
      "Model\n",
      " Size\n",
      ":**\n",
      " You\n",
      " need\n",
      " to\n",
      " store\n",
      " a\n",
      " large\n",
      " model\n",
      " file\n",
      " locally\n",
      " (\n",
      "often\n",
      " many\n",
      " GB\n",
      "s\n",
      ").\n",
      " Sm\n",
      "aller\n",
      " models\n",
      " might\n",
      " be\n",
      " run\n",
      " more\n",
      " easily\n",
      " and\n",
      " efficiently\n",
      " offline\n",
      ".\n",
      "\n",
      "\n",
      "In\n",
      " summary\n",
      ",\n",
      " the\n",
      " primary\n",
      " advantages\n",
      " of\n",
      " running\n",
      " an\n",
      " L\n",
      "LM\n",
      " locally\n",
      " are\n",
      " enhanced\n",
      " privacy\n",
      "/security\n",
      ",\n",
      " complete\n",
      " control\n",
      " over\n",
      " parameters\n",
      " and\n",
      " behavior\n",
      ",\n",
      " customization\n",
      " flexibility\n",
      ",\n",
      " and\n",
      " the\n",
      " ability\n",
      " to\n",
      " use\n",
      " it\n",
      " completely\n",
      " offline\n",
      ".\n",
      " The\n",
      " main\n",
      " disadvantages\n",
      " involve\n",
      " substantial\n",
      " hardware\n",
      " requirements\n",
      " and\n",
      " continuous\n",
      " resource\n",
      " consumption\n",
      " by\n",
      " the\n",
      " model\n",
      " itself\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 1 way of doing it\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"placeholder\", \"{msg}\")\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"msg\": [HumanMessage(\"What is the advantage of running LLM in local machine\")]})\n",
    "\n",
    "for str in llm.stream(prompt):\n",
    "    print(str.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv313 (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
